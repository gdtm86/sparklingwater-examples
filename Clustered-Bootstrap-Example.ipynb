{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Bootstrap\n",
    "\n",
    "In this notebook, we will look at a different bootstrap technique called 'Clustered Bootstrap'. Here sampling is done at a higher unit than merged onto the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the Spark and SparkSQL context have started successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f815b520650>\n",
      "<pyspark.sql.context.HiveContext object at 0x7f815b500e90>\n",
      "<pyspark.sql.context.HiveContext object at 0x7f815b500e90>\n"
     ]
    }
   ],
   "source": [
    "print sc\n",
    "print sqlContext\n",
    "print sqlCtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = (sqlContext.read\n",
    "           .format('com.databricks.spark.csv')\n",
    "           .option(\"header\", \"true\") # Use first line of all files as header\n",
    "           .option(\"inferSchema\", \"true\") # Automatically infer data types\n",
    "           .load(\"skewdata-policy-new.csv\")\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('policyid', 'int'), ('age', 'int'), ('values', 'double')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------+\n",
      "|policyid|age|     values|\n",
      "+--------+---+-----------+\n",
      "|       0| 10|81.37291811|\n",
      "|       0| 10|25.70097086|\n",
      "|       0| 10|4.942646012|\n",
      "|       1| 11|43.02085256|\n",
      "|       1| 11|81.69058902|\n",
      "|       1| 11|51.19523649|\n",
      "|       2| 12|55.65990905|\n",
      "|       2| 12|15.15315474|\n",
      "|       2| 12|38.74578007|\n",
      "|       3| 13|12.61038468|\n",
      "|       3| 13|22.41509375|\n",
      "|       3| 13| 18.3557207|\n",
      "|       4| 14|38.08150137|\n",
      "|       4| 14|48.17113476|\n",
      "|       4| 14|18.46272527|\n",
      "|       5| 15|44.64225129|\n",
      "|       5| 15|25.39108197|\n",
      "|       5| 15|20.41087394|\n",
      "|       6| 16|15.77818657|\n",
      "|       6| 16|19.35148454|\n",
      "+--------+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above dataset, we need to sample it in such a way that all the records for a particular policy id need to be included in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First find the distinct policy ids from the dataset and have them stored in a different dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policyids = data_df.select('policyid').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|policyid|\n",
      "+--------+\n",
      "|       0|\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policyids.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a function the creates specified number of samples based on the sample size specified using fraction\n",
    "\n",
    "def clusteredSamples(data,policies,policyid_sample_fraction,num_of_samples):\n",
    "    \n",
    "    #Initiate an emtpy sample list\n",
    "    samples = []\n",
    "    \n",
    "    for n in range(0,num_of_samples):\n",
    "        \n",
    "        #Create a sample of the unique policy ids\n",
    "        policyids_sample = policies.sample(withReplacement=False, fraction=policyid_sample_fraction)\n",
    "    \n",
    "        #Sample the data based on the sampled policyids\n",
    "        sample = policyids_sample.join(data,on='policyid',how='inner')\n",
    "        \n",
    "        #Add the sample to the samples list\n",
    "        samples.append(sample)\n",
    "        \n",
    "    #We will return a list of clustered samples\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleList = clusteredSamples(data_df,policyids,0.8,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a linear regression on all the samples in samplesList sequentially and save the coffeccients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def runLinearRegression(samples):\n",
    "    #initiate a result list\n",
    "    samples_coefficients = []\n",
    "    \n",
    "    #Create a vector Assembler\n",
    "    feature_columns = ['age']\n",
    "    vectorAssembler = VectorAssembler(inputCols = feature_columns, outputCol = 'features_vector')\n",
    "    \n",
    "    #Create a linear regresson model\n",
    "    lr = LinearRegression(featuresCol ='features_vector', \n",
    "                          labelCol = 'values',\n",
    "                          predictionCol = 'predicted_values',\n",
    "                          maxIter=5, \n",
    "                          elasticNetParam = 0.5,\n",
    "                          solver=\"l-bfgs\")\n",
    "    for i in range(0,len(samples)):\n",
    "        sample_df = samples[i]\n",
    "        sample_df1 = vectorAssembler.transform(sample_df)\n",
    "        \n",
    "        #Fit the linear Regression model\n",
    "        sample_lr = lr.fit(sample_df1)\n",
    "        \n",
    "        #Save the coefficients from the Regression model\n",
    "        samples_coefficients.append(sample_lr.coefficients)\n",
    "    \n",
    "    #Return the list of coefficients from running glm on each sample set    \n",
    "    return samples_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleCoefficients = runLinearRegression(sampleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-2.5583]),\n",
       " DenseVector([-2.3188]),\n",
       " DenseVector([-2.3443]),\n",
       " DenseVector([-2.1986]),\n",
       " DenseVector([-2.6441]),\n",
       " DenseVector([-2.3188]),\n",
       " DenseVector([-1.8888]),\n",
       " DenseVector([-2.3443]),\n",
       " DenseVector([-2.1632]),\n",
       " DenseVector([-3.4732]),\n",
       " DenseVector([-3.6289]),\n",
       " DenseVector([-3.4164]),\n",
       " DenseVector([-2.6553]),\n",
       " DenseVector([-2.7576]),\n",
       " DenseVector([-2.3557]),\n",
       " DenseVector([-2.8209]),\n",
       " DenseVector([-2.6926]),\n",
       " DenseVector([-2.3467]),\n",
       " DenseVector([-2.4428]),\n",
       " DenseVector([-2.8209])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleCoefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
