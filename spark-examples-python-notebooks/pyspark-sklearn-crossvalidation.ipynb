{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will look at how to use spark-sklearn project to run distributed cross validation of the Scikit-learn models. \n",
    "\n",
    "Since Scikit-learn models are by design not distributed, this will help us use spark cluster's several cpu resources to run cross validation. This is similar to the scikit-learn's joblib for multicore parallelism - https://pythonhosted.org/joblib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f3572e70750>\n",
      "<pyspark.sql.context.HiveContext object at 0x7f3572e5b050>\n",
      "<pyspark.sql.context.HiveContext object at 0x7f3572e5b050>\n"
     ]
    }
   ],
   "source": [
    "print sc\n",
    "print sqlContext\n",
    "print sqlCtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a RandomForestClassifier on the digits dataset in Scikit-learn and use Scikit-learn's GridSearch cross validation to fit the best model. \n",
    "\n",
    "This is a bit slower as the cross validation is happening on a single server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'bootstrap': [True, False], 'min_samples_leaf': [1, 3, 10], 'n_estimators': [10, 20, 40, 80], 'min_samples_split': [1, 3, 10], 'criterion': ['gini', 'entropy'], 'max_features': [1, 3, 10], 'max_depth': [3, None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import grid_search, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"n_estimators\": [10, 20, 40, 80]}\n",
    "gs = grid_search.GridSearchCV(RandomForestClassifier(), param_grid=param_grid)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will import the spark-sklearn package and run scikit-learn's RandomForestClassifier on digits dataset and use Spark-sklearn's GridSearchCV cross validation to fit the best model\n",
    "\n",
    "This will help you leverage multiple cores and memory available on the large spark cluster to run grid search on a very large parameter grid. This will also run faster compared to the single thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the only change in that we need to import the GridSearchCV from spark_learn instead of sklear.grid_search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'bootstrap': [True, False], 'min_samples_leaf': [1, 3, 10], 'n_estimators': [10, 20, 40, 80], 'min_samples_split': [1, 3, 10], 'criterion': ['gini', 'entropy'], 'max_features': [1, 3, 10], 'max_depth': [3, None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import grid_search, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "# Use spark_sklearnâ€™s grid search instead:\n",
    "from spark_sklearn import GridSearchCV\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"n_estimators\": [10, 20, 40, 80]}\n",
    "gs = grid_search.GridSearchCV(RandomForestClassifier(), param_grid=param_grid)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of parallelizing scikit-learns cross validation, Spark-sklearn has some other features like converting between scikit-learn models and spark mllib models.\n",
    "\n",
    "More on this here.\n",
    "\n",
    "http://pythonhosted.org/spark-sklearn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
