{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BootStrap Example\n",
    "\n",
    "In this example, we will perform bookstrapping technique on a synthetic dataset 'skewdata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the Spark and SparkSQL context have started successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f02d38dc650>\n",
      "<pyspark.sql.context.HiveContext object at 0x7f02d38bce90>\n",
      "<pyspark.sql.context.HiveContext object at 0x7f02d38bce90>\n"
     ]
    }
   ],
   "source": [
    "print sc\n",
    "print sqlContext\n",
    "print sqlCtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = (sqlContext.read\n",
    "                  .format('com.databricks.spark.csv')\n",
    "                  .option(\"header\", \"true\") # Use first line of all files as header\n",
    "                  .option(\"inferSchema\", \"true\") # Automatically infer data types\n",
    "                  .load(\"skewdata.csv\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|     values|\n",
      "+-----------+\n",
      "|81.37291811|\n",
      "|25.70097086|\n",
      "|4.942646012|\n",
      "|43.02085256|\n",
      "|81.69058902|\n",
      "|51.19523649|\n",
      "|55.65990905|\n",
      "|15.15315474|\n",
      "|38.74578007|\n",
      "|12.61038468|\n",
      "|22.41509375|\n",
      "| 18.3557207|\n",
      "|38.08150137|\n",
      "|48.17113476|\n",
      "|18.46272527|\n",
      "|44.64225129|\n",
      "|25.39108197|\n",
      "|20.41087394|\n",
      "|15.77818657|\n",
      "|19.35148454|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to find the confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of a column in Spark DataFrame \n",
    "```\n",
    "data_df.selectExpr(\"avg(values) as mean\").collect()[0].asDict().get('mean')\n",
    "```\n",
    "Alternative way to calculate the mean\n",
    "```\n",
    "data_df.select(avg(\"values\")).collect()[0].asDict().get('avg(values)')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to get confidence interval\n",
    "\n",
    "def getConfidenceInterval(inputDataFrame,num_of_samples, left_quantile_fraction, right_quantile_fraction):\n",
    "    #Simulate by sampling and calculating averages for each subsamples\n",
    "    sample_means = np.empty([num_of_samples])\n",
    "    for n in range(0,num_of_samples):\n",
    "        sample_means[n] = (inputDataFrame.sample(withReplacement = True, fraction=1.0)\n",
    "                   .selectExpr(\"avg(values) as mean\")\n",
    "                   .collect()[0]\n",
    "                   .asDict()\n",
    "                   .get('mean'))\n",
    "            \n",
    "    ## Sort the means\n",
    "    sample_means.sort()\n",
    "    \n",
    "    ## Create a Pandas Dataframe from the numpy array\n",
    "    sampleMeans_local_df = pd.DataFrame(sample_means)\n",
    "    \n",
    "    ## Create a Spark Dataframe from the pandas dataframe\n",
    "    fields = [StructField(\"mean_values\", DoubleType(), True)]\n",
    "    schema = StructType(fields)\n",
    "    sampleMeans_df = sqlContext.createDataFrame(sampleMeans_local_df, schema)\n",
    "    \n",
    "    ## Calculate the left_quantile and right_quantiles \n",
    "    sqlContext.registerDataFrameAsTable(sampleMeans_df, 'Guru_SampleMeansTable')\n",
    "    quantiles_df = sqlContext.sql(\"select percentile(cast(mean_values as bigint),\"\n",
    "                                  \"array(\"+str(left_quantile_fraction)+\",\"+str(right_quantile_fraction)+\")) as \"\n",
    "                                  \"percentiles from Guru_SampleMeansTable\")\n",
    "    return quantiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get 95% confidence interval in a two-tailed hypothesis testing\n",
    "quantiles_df = getConfidenceInterval(data_df, 1000, 0.025, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "| percentiles|\n",
      "+------------+\n",
      "|[24.0, 37.0]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## We can now look at these percentiles and determine the critical region of sampling distribution\n",
    "quantiles_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
